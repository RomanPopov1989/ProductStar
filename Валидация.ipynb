{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Валидации\n",
    "\n",
    "Давайте теперь поговорим об общей концепции валидации, это одна из ключевых проблем во всем машинном обучении, Если говорить коротко, то мы хотим проверить, __работает ли модель так же качественно на новых, ранее невидимых данных__. Проблема заключается в том, что качество модели может отличаться на обучающих данных, которые являются историческими, и на невидимых тестовых данных из будущего. Мы хотим, чтобы модель могла выявить закономернности в данных, которые хорошо обобщаются как на обучающей выборке, так и на валидационной выборке, и на тестовой выборке. \n",
    "\n",
    "__Недообучение__ - проблема, при которой качество плохое и на обучающей выборке, и на тестовой (левая картина).\n",
    "\n",
    "__Переобучение__ - пробелма, при которой качество хорошее на обучающей выборке и плохое на тестовой выборке (правая картина).\n",
    "\n",
    "__Наша цель__ - получить некоторый баланс между переобучением и недообучением (средняя картина).\n",
    "\n",
    "<img src=\"images/web2_fit.png\" width=800 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.093</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.389</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2  var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.093  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.389  12.3622  7.0433  5.6208   \n",
       "\n",
       "     var_7  ...  var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  18.6266  ...   4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n",
       "1  16.5338  ...   7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   8.1267   8.7889  18.3560   1.9518  \n",
       "\n",
       "[2 rows x 202 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    \"./santander-customer-transaction-prediction/train.csv\", nrows=60000\n",
    ")\n",
    "data.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Валидация на отложенной выборке\n",
    "\n",
    "Самая простая стратегия, которую мы можем использовать, это разделить данные на две части: для обучения и для проверки. На первой части выборки, мы будем обучаать модель, и это обучающая выборка, а на второй части выборки -\n",
    "проверим качество работы модели. Мы хотели бы, чтобы метрики качества на обеих метриках были близки друг к другу.\n",
    "\n",
    "<img src=\"images/web2_holdout_2sample.png\" width=600 height=300 />\n",
    "\n",
    "Для использования этого типа валидации будем использовать функцию `train_test_split` из модуля `sklearn.model_selection`. \n",
    "\n",
    "У функции есть несколько важных параметров:\n",
    "\n",
    "* `train_size` - доля или количество объектов в обучающей выборке;\n",
    "* `shuffle` - бинарный флаг предварительного перемешивания данных;\n",
    "* `random_state` - состояние случайного счетчика, будем его использовать для получения одинаковых разбиений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = 45000 rows, 200 cols\n",
      "x_test.shape = 15000 rows, 200 cols\n"
     ]
    }
   ],
   "source": [
    "# Разобьем данные на train / test\n",
    "\n",
    "x_train, x_test = train_test_split(\n",
    "    data.drop([\"ID_code\", \"target\"], axis=1), train_size=0.75, shuffle=True, random_state=1,\n",
    ")\n",
    "y_train, y_test = train_test_split(\n",
    "    data[\"target\"], train_size=0.75, shuffle=True, random_state=1,\n",
    ")\n",
    "print(\"x_train.shape = {} rows, {} cols\".format(*x_train.shape))\n",
    "print(\"x_test.shape = {} rows, {} cols\".format(*x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaling',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('model',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=27,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создадим конвейер для автоматического \n",
    "# масштабирование признаков и использования модели логистической регрессии\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaling\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(random_state=27))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-score: 0.864, Test-score: 0.846\n"
     ]
    }
   ],
   "source": [
    "# Оценим значение метрики качества на обучении и на тесте\n",
    "\n",
    "train_score = roc_auc_score(y_train, pipeline.predict_proba(x_train)[:, 1])\n",
    "test_score = roc_auc_score(y_test, pipeline.predict_proba(x_test)[:, 1])\n",
    "\n",
    "print(f\"Train-score: {round(train_score, 3)}, Test-score: {round(test_score, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили первые результаты, но насколько мы можем доверять таким результатам?\n",
    "Дело в том, что в ходе построения модели машинного обучения, мы будем проводить много экспериментов и проверять качество модели на одной и той же выборке не очень хороший подход. Если мы будем оценивать качество модели на одной и той же выборке много раз, мы можем слишком сильно настроиться на эту выборку или __переобучиться под конкретную выборку__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Валидация на отложенной и тестовой выборках\n",
    "\n",
    "Хорошим решением такой проблемы, будет разбиение исходного набора данных на три части: обучающую, валидационную и тестовую. На обучающей выборке мы будем обучать модель, на валидационной - оценивать качество модели на каждом эксперименте, а тестовую выборку будем использовать в конце работы над проектом для окончательной проверки качества модели.\n",
    "\n",
    "<img src=\"images/web2_holdout_3sample.png\" width=600 height=300 />\n",
    "\n",
    "Для реализации такого типа валидации __дважды воспользуемся__ функцией `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = 42000 rows, 200 cols\n",
      "x_valid.shape = 12600 rows, 200 cols\n",
      "x_test.shape = 5400 rows, 200 cols\n"
     ]
    }
   ],
   "source": [
    "# Разобьем данные на train / valid\n",
    "\n",
    "x_train, x_valid = train_test_split(\n",
    "    data.drop([\"ID_code\", \"target\"], axis=1), train_size=0.7, shuffle=True, random_state=1,\n",
    ")\n",
    "y_train, y_valid = train_test_split(\n",
    "    data[\"target\"], train_size=0.7, shuffle=True, random_state=1,\n",
    ")\n",
    "\n",
    "# Разобьем данные на valid / test\n",
    "\n",
    "x_valid, x_test = train_test_split(\n",
    "    x_valid, train_size=0.7, shuffle=True, random_state=27\n",
    ")\n",
    "y_valid, y_test = train_test_split(\n",
    "    y_valid, train_size=0.7, shuffle=True, random_state=27\n",
    ")\n",
    "\n",
    "print(\"x_train.shape = {} rows, {} cols\".format(*x_train.shape))\n",
    "print(\"x_valid.shape = {} rows, {} cols\".format(*x_valid.shape))\n",
    "print(\"x_test.shape = {} rows, {} cols\".format(*x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaling',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('model',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=27,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-score: 0.864, Valid-score: 0.853, Test-score: 0.836\n"
     ]
    }
   ],
   "source": [
    "train_score = roc_auc_score(y_train, pipeline.predict_proba(x_train)[:, 1])\n",
    "valid_score = roc_auc_score(y_valid, pipeline.predict_proba(x_valid)[:, 1])\n",
    "test_score = roc_auc_score(y_test, pipeline.predict_proba(x_test)[:, 1])\n",
    "\n",
    "print(f\"Train-score: {round(train_score, 3)}, Valid-score: {round(valid_score, 3)}, Test-score: {round(test_score, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили более устойчивые результаты, которые позволят более честно оценить качество модели и использовать текцщее решенеи, как отправную точку для дальнейшего улучшения решения. Проблема нашего подхода заключается в том, что мы используем не все данные для обучения и для тестирования модели. Возможно, что разбиение, которое мы провели является слишком специфичным и не позволяет оценить качество модели объективно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold кросс-валидация\n",
    "\n",
    "Рассмотрим другой подход, которая называется __K-Fold__ валидацией. В то же время, основное преимущество K-Fold валидации состоит в том, что мы используем каждый независимый блок для проверки только один раз. Этот метод валидации - хороший выбор, когда у нас есть относительно небольшой объем данных, и мы можем получить достаточно большое отличие в качестве на каждом блоке.\n",
    "\n",
    "<img src=\"images/web4_kfold.png\" width=800 height=600 />\n",
    "\n",
    "Для проведения кросс-валидации будем использовать генератор `KFold` и функцию `cross_val_score`.\n",
    "\n",
    "Генератор `KFold` принимает несколько параметров:\n",
    "* `n_splits` - количество групп, на которые разбить данные (на картинке их 10);\n",
    "* `shuffle` - бинарный флаг предварительного перемешивания данных;\n",
    "* `random_state` - состояние случайного счетчика, будем его использовать для получения одинаковых разбиений.\n",
    "\n",
    "Функция `cross_val_score` принимает следующие параметров:\n",
    "* `estimator` - модель, которую мы хотим обучать;\n",
    "* `X`, `y` - матрица признаков и вектор целевой переменной для обучения модели;\n",
    "* `scoring` - название метрики качества для расчета на каждой итерации обучения;\n",
    "* `cv` - объект `KFold` или просто количество блоков, которые использовать для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-results: 0.8553 +/- 0.007\n"
     ]
    }
   ],
   "source": [
    "# определяем стратегию для проведения кросс-валидации\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=27)\n",
    "\n",
    "# проводим кросс-валидацию\n",
    "cv = cross_val_score(\n",
    "    estimator=pipeline,\n",
    "    X=data.drop([\"ID_code\", \"target\"], axis=1),\n",
    "    y=data[\"target\"],\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=kfold\n",
    ")\n",
    "\n",
    "# Считаем среднее значение метрики на каждом фолде и выводим среднее значение\n",
    "print(f\"CV-results: {round(np.mean(cv), 4)} +/- {round(np.std(cv), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85765454 0.86016764 0.84786461 0.85616674 0.85565036 0.86268263\n",
      " 0.85306287 0.85539241 0.8640899  0.84032007]\n"
     ]
    }
   ],
   "source": [
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе, мы получаем значение метрики качества на каждом тестовом фолде в отдельности, и для получения итогового качества можем посчитать среднее значение на всех фолдах и стандартное отклонение на каждом фолде.\n",
    "\n",
    "## Stratified KFold валидация\n",
    "\n",
    "Способ валидации очень похожий на KFold, но разделение данных на фолды происходит так, чтобы в каждом фолде было одинаковое распределение целевой переменной. Такой подход полезен, если у нас слишком мало данных для обучения модели, а также для сильно несбалансированных наборов данных для задачи классификации.\n",
    "\n",
    "<img src=\"images/web2_stratified_kfold.png\" width=600 height=300 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-results: 0.8553 +/- 0.007\n"
     ]
    }
   ],
   "source": [
    "# определяем стратегию для проведения кросс-валидации\n",
    "skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=27)\n",
    "\n",
    "# проводим кросс-валидацию\n",
    "cv = cross_val_score(\n",
    "    estimator=pipeline,\n",
    "    X=data.drop([\"ID_code\", \"target\"], axis=1),\n",
    "    y=data[\"target\"],\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=kfold\n",
    ")\n",
    "\n",
    "# Считаем среднее значение метрики на каждом фолде и выводим среднее значение\n",
    "print(f\"CV-results: {round(np.mean(cv), 4)} +/- {round(np.std(cv), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85765454 0.86016764 0.84786461 0.85616674 0.85565036 0.86268263\n",
      " 0.85306287 0.85539241 0.8640899  0.84032007]\n"
     ]
    }
   ],
   "source": [
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Комбинация отложенной выборки и кросс-валидации\n",
    "\n",
    "Если есть возможность (вычислительная и по объему наблюдений) имеет смысл комбинировать разные способы валидации, например, это может быть комбинация кросс-валидации и отложенной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем отложенную выборку\n",
    "x_train, x_test = train_test_split(\n",
    "    data.drop([\"ID_code\", \"target\"], axis=1), train_size=0.7, shuffle=True, random_state=1,\n",
    ")\n",
    "y_train, y_test = train_test_split(\n",
    "    data[\"target\"], train_size=0.7, shuffle=True, random_state=1,\n",
    ")\n",
    "\n",
    "# фирксируем стратегию кросс-валидации\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-results: 0.8571 +/- 0.01\n"
     ]
    }
   ],
   "source": [
    "# проводим кросс-валидацию\n",
    "cv = cross_val_score(\n",
    "    estimator=pipeline,\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=kfold\n",
    ")\n",
    "\n",
    "# Считаем среднее значение метрики на каждом фолде и выводим среднее значение\n",
    "print(f\"CV-results: {round(np.mean(cv), 4)} +/- {round(np.std(cv), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-Of-Fold-score: 0.848\n"
     ]
    }
   ],
   "source": [
    "# Обучаем отдельно пайплайн на обучающих данных\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "# Делаем прогнозы на отложенной выборке\n",
    "y_pred = pipeline.predict_proba(x_test)[:, 1]\n",
    "score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Out-Of-Fold-score: {round(score, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta between CV-score and OOF-score: 0.0093\n"
     ]
    }
   ],
   "source": [
    "delta = np.mean(cv) - score\n",
    "print(f\"Delta between CV-score and OOF-score: {round(delta, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разница между значением метрики на кросс-валидации и значением метрики на отложенной выборке очень маленькое, это говорит о том, что мы получили устойчивые значения проверки модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашка\n",
    "\n",
    "__Датка:__ `ieee-fraud-detection`\n",
    "__Метрика качества:__ `ROC-AUC`\n",
    "\n",
    "__Задание 1:__ загрузить датасет, посчитать количество пропусков в признаках, выбрать только те признаки, которые не имеют пропущенные значение. Сколько таких признаков?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2__: разбить данные на обучающую и тестовую часть в соотношении 70 / 30, обучить модель линейной регрессии с предварительным масштабированием признаков с помощью `StandartScaler`, оценить качество на тестовой части с помощью метрики `ROC-AUC`. Для построения модели не использовать признаки `'TransactionAmt', 'card1', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1'`. При разбиении данных зафиксировать `random_state = 1`. \n",
    "\n",
    "__Ответ округлить__ с точностью до 3-го знака."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 3__: разбить данные на обучении / валидацию / тест в соотношении 50% / 25% / 25%, от размера исходного датасета. Зафиксировать `random_state = 1` для обоих разбиений. Оценить разницу в метрике качества на валидационной и на тестовой части. \n",
    "\n",
    "__Ответ округлить__ с точностью до 4-го знака."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 4:__ выполнить `KFold` кросс-валидацию на 5-блоков, с перемешиванием данных, зафиксировать `random_state = 27`. Оценить среднее значение метрики качества. \n",
    "\n",
    "__Ответ округлить__ с точностью до 4-го знака."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 5:__ для задания 4 оценить стандартное отклонение метрики качества по фолдам.\n",
    "\n",
    "__Ответ округлить__ с точностью до 3-го знака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>...</th>\n",
       "      <th>V330</th>\n",
       "      <th>V331</th>\n",
       "      <th>V332</th>\n",
       "      <th>V333</th>\n",
       "      <th>V334</th>\n",
       "      <th>V335</th>\n",
       "      <th>V336</th>\n",
       "      <th>V337</th>\n",
       "      <th>V338</th>\n",
       "      <th>V339</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.500</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.000</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.000</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.000</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.000</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>3086995</td>\n",
       "      <td>0</td>\n",
       "      <td>2005989</td>\n",
       "      <td>55.385</td>\n",
       "      <td>C</td>\n",
       "      <td>2256</td>\n",
       "      <td>545.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>3086996</td>\n",
       "      <td>0</td>\n",
       "      <td>2006074</td>\n",
       "      <td>117.000</td>\n",
       "      <td>W</td>\n",
       "      <td>2518</td>\n",
       "      <td>555.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>3086997</td>\n",
       "      <td>0</td>\n",
       "      <td>2006135</td>\n",
       "      <td>50.000</td>\n",
       "      <td>S</td>\n",
       "      <td>2748</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>228.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>3086998</td>\n",
       "      <td>0</td>\n",
       "      <td>2006177</td>\n",
       "      <td>100.000</td>\n",
       "      <td>H</td>\n",
       "      <td>16075</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>3086999</td>\n",
       "      <td>0</td>\n",
       "      <td>2006364</td>\n",
       "      <td>107.950</td>\n",
       "      <td>W</td>\n",
       "      <td>4436</td>\n",
       "      <td>174.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 394 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0            2987000        0          86400          68.500         W  13926   \n",
       "1            2987001        0          86401          29.000         W   2755   \n",
       "2            2987002        0          86469          59.000         W   4663   \n",
       "3            2987003        0          86499          50.000         W  18132   \n",
       "4            2987004        0          86506          50.000         H   4497   \n",
       "...              ...      ...            ...             ...       ...    ...   \n",
       "99995        3086995        0        2005989          55.385         C   2256   \n",
       "99996        3086996        0        2006074         117.000         W   2518   \n",
       "99997        3086997        0        2006135          50.000         S   2748   \n",
       "99998        3086998        0        2006177         100.000         H  16075   \n",
       "99999        3086999        0        2006364         107.950         W   4436   \n",
       "\n",
       "       card2  card3       card4  card5  ... V330  V331   V332  V333  V334  \\\n",
       "0        NaN  150.0    discover  142.0  ...  NaN   NaN    NaN   NaN   NaN   \n",
       "1      404.0  150.0  mastercard  102.0  ...  NaN   NaN    NaN   NaN   NaN   \n",
       "2      490.0  150.0        visa  166.0  ...  NaN   NaN    NaN   NaN   NaN   \n",
       "3      567.0  150.0  mastercard  117.0  ...  NaN   NaN    NaN   NaN   NaN   \n",
       "4      514.0  150.0  mastercard  102.0  ...  0.0   0.0    0.0   0.0   0.0   \n",
       "...      ...    ...         ...    ...  ...  ...   ...    ...   ...   ...   \n",
       "99995  545.0  185.0        visa  226.0  ...  NaN   NaN    NaN   NaN   NaN   \n",
       "99996  555.0  150.0        visa  226.0  ...  NaN   NaN    NaN   NaN   NaN   \n",
       "99997  111.0  150.0        visa  228.0  ...  0.0   0.0  125.0  75.0   0.0   \n",
       "99998  514.0  150.0  mastercard  102.0  ...  0.0   0.0    0.0   0.0   0.0   \n",
       "99999  174.0  150.0        visa  226.0  ...  NaN   NaN    NaN   NaN   NaN   \n",
       "\n",
       "        V335  V336  V337  V338  V339  \n",
       "0        NaN   NaN   NaN   NaN   NaN  \n",
       "1        NaN   NaN   NaN   NaN   NaN  \n",
       "2        NaN   NaN   NaN   NaN   NaN  \n",
       "3        NaN   NaN   NaN   NaN   NaN  \n",
       "4        0.0   0.0   0.0   0.0   0.0  \n",
       "...      ...   ...   ...   ...   ...  \n",
       "99995    NaN   NaN   NaN   NaN   NaN  \n",
       "99996    NaN   NaN   NaN   NaN   NaN  \n",
       "99997  125.0  75.0   0.0   0.0   0.0  \n",
       "99998    0.0   0.0   0.0   0.0   0.0  \n",
       "99999    NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[100000 rows x 394 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    \"data.csv\"\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00000    75\n",
       "0.60417    50\n",
       "0.61608    46\n",
       "0.00011    32\n",
       "0.68359    29\n",
       "0.30933    23\n",
       "0.30239    22\n",
       "0.34052    20\n",
       "0.46178    18\n",
       "0.68229    18\n",
       "0.59534    16\n",
       "0.71208    12\n",
       "0.83961     3\n",
       "0.68553     3\n",
       "0.78289     2\n",
       "0.09037     2\n",
       "0.65327     1\n",
       "0.45587     1\n",
       "0.00003     1\n",
       "0.56598     1\n",
       "0.00007     1\n",
       "0.00525     1\n",
       "0.00004     1\n",
       "0.69653     1\n",
       "0.91769     1\n",
       "0.15393     1\n",
       "0.46175     1\n",
       "0.68924     1\n",
       "0.87259     1\n",
       "0.57670     1\n",
       "0.57633     1\n",
       "0.34047     1\n",
       "0.88847     1\n",
       "0.90479     1\n",
       "0.89495     1\n",
       "0.01354     1\n",
       "0.30926     1\n",
       "0.93928     1\n",
       "0.65454     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data[['TransactionAmt', 'card1', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = 70000 rows, 17 cols\n",
      "x_test.shape = 30000 rows, 17 cols\n"
     ]
    }
   ],
   "source": [
    "# Разобьем данные на train / test\n",
    "\n",
    "x_train, x_test = train_test_split(\n",
    "    data[['TransactionAmt', 'card1', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1']],    train_size=0.7, shuffle=True, random_state=1,\n",
    ")\n",
    "y_train, y_test = train_test_split(\n",
    "    data[\"isFraud\"], train_size=0.7, shuffle=True, random_state=1,\n",
    ")\n",
    "print(\"x_train.shape = {} rows, {} cols\".format(*x_train.shape))\n",
    "print(\"x_test.shape = {} rows, {} cols\".format(*x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaling&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;, LogisticRegression(random_state=27))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaling&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;, LogisticRegression(random_state=27))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=27)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaling', StandardScaler()),\n",
       "                ('model', LogisticRegression(random_state=27))])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создадим конвейер для автоматического \n",
    "# масштабирование признаков и использования модели логистической регрессии\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaling\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(random_state=27))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-score: 0.711, Test-score: 0.709\n"
     ]
    }
   ],
   "source": [
    "# Оценим значение метрики качества на обучении и на тесте\n",
    "\n",
    "train_score = roc_auc_score(y_train, pipeline.predict_proba(x_train)[:, 1])\n",
    "test_score = roc_auc_score(y_test, pipeline.predict_proba(x_test)[:, 1])\n",
    "\n",
    "print(f\"Train-score: {round(train_score, 3)}, Test-score: {round(test_score, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = 50000 rows, 17 cols\n",
      "x_valid.shape = 25000 rows, 17 cols\n",
      "x_test.shape = 25000 rows, 17 cols\n"
     ]
    }
   ],
   "source": [
    "# Разобьем данные на train / valid\n",
    "\n",
    "x_train, x_valid = train_test_split(\n",
    "    df1, train_size=0.5, random_state=1,\n",
    ")\n",
    "y_train, y_valid = train_test_split(\n",
    "    data[\"isFraud\"], train_size=0.5, random_state=1,\n",
    ")\n",
    "\n",
    "# Разобьем данные на valid / test\n",
    "\n",
    "x_valid, x_test = train_test_split(\n",
    "    x_valid, train_size=0.5, random_state=1\n",
    ")\n",
    "y_valid, y_test = train_test_split(\n",
    "    y_valid, train_size=0.5, random_state=1\n",
    ")\n",
    "\n",
    "print(\"x_train.shape = {} rows, {} cols\".format(*x_train.shape))\n",
    "print(\"x_valid.shape = {} rows, {} cols\".format(*x_valid.shape))\n",
    "print(\"x_test.shape = {} rows, {} cols\".format(*x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaling&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;, LogisticRegression(random_state=27))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaling&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;, LogisticRegression(random_state=27))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-33\" type=\"checkbox\" ><label for=\"sk-estimator-id-33\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=27)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaling', StandardScaler()),\n",
       "                ('model', LogisticRegression(random_state=27))])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-score: 0.7069, Valid-score: 0.7057, Test-score: 0.7173\n",
      "0.0116\n"
     ]
    }
   ],
   "source": [
    "train_score = roc_auc_score(y_train, pipeline.predict_proba(x_train)[:, 1])\n",
    "valid_score = roc_auc_score(y_valid, pipeline.predict_proba(x_valid)[:, 1])\n",
    "test_score = roc_auc_score(y_test, pipeline.predict_proba(x_test)[:, 1])\n",
    "\n",
    "print(f\"Train-score: {round(train_score, 4)}, Valid-score: {round(valid_score, 4)}, Test-score: {round(test_score, 4)}\")\n",
    "print(round(test_score - valid_score,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-results: 0.022 +/- 0.022\n"
     ]
    }
   ],
   "source": [
    "# определяем стратегию для проведения кросс-валидации\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=27)\n",
    "\n",
    "# проводим кросс-валидацию\n",
    "cv = cross_val_score(\n",
    "    estimator=pipeline,\n",
    "    X=df1,\n",
    "    y=data[\"isFraud\"],\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=kfold\n",
    ")\n",
    "\n",
    "# Считаем среднее значение метрики на каждом фолде и выводим среднее значение\n",
    "print(f\"CV-results: {round(np.std(cv), 3)} +/- {round(np.std(cv), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
